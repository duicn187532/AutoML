import streamlit as st
import pandas as pd
import numpy as np
import json
import joblib
import io
import matplotlib.pyplot as plt
import matplotlib

from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, mean_squared_error

from flaml import AutoML
from flaml.automl.data import get_output_from_log

# -------- ç‰ˆé¢ & å­—å‹ --------
matplotlib.rcParams['font.family'] = 'sans-serif'
st.set_page_config(page_title="AutoML å°å¹«æ‰‹", layout="wide")
st.title("ğŸ¤– AutoML å°å¹«æ‰‹ (FLAML + Streamlit)")

# -------- Session ç‹€æ…‹ --------
for key in ["model", "X_columns", "scaler", "train_done", "log_file", "column_types", "categorical_columns"]:
    if key not in st.session_state:
        st.session_state[key] = None

# é è¨­ log æª”åï¼ˆæ¯æ¬¡è¨“ç·´æœƒè¦†å¯«ï¼‰
if not st.session_state["log_file"]:
    st.session_state["log_file"] = "flaml_training.log"

# -------- åˆ†é  --------
tab1, tab2, tab3 = st.tabs(["ğŸ§ª æ¨¡å‹è¨“ç·´", "ğŸ“¥ çµæœä¸‹è¼‰", "ğŸ” æ¨¡å‹é æ¸¬"])

# === ğŸ§ª æ¨¡å‹è¨“ç·´ ===
with tab1:
    st.header("ğŸ§ª æ¨¡å‹è¨“ç·´å€")
    uploaded_file = st.file_uploader("è«‹ä¸Šå‚³è¨“ç·´ç”¨ CSV æª”", type=["csv"])

    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        # æ¸…ç†æ¬„ä½åç¨±ï¼ˆå»é™¤ç©ºæ ¼ï¼‰
        df.columns = df.columns.str.strip()
        st.subheader("ğŸ“Š è³‡æ–™é è¦½")
        st.dataframe(df.head())

        task = st.selectbox("ğŸ§  é æ¸¬ä»»å‹™é¡å‹", options=["classification", "regression"])
        target = st.selectbox("ğŸ¯ é¸æ“‡ç›®æ¨™æ¬„ä½ï¼ˆYï¼‰", options=df.columns)

        # ===== æ¬„ä½å‹åˆ¥è¨­å®š =====
        st.subheader("ğŸ›  æ¬„ä½å‹åˆ¥è¨­å®š")
        column_types = {}
        categorical_columns = []  # è¨˜éŒ„éœ€è¦åš one-hot çš„æ¬„ä½
        
        for col in df.drop(columns=[target]).columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                default_type = "numeric"
            else:
                default_type = "category"
            col_type = st.selectbox(
                f"{col} æ¬„ä½å‹åˆ¥", ["numeric", "category"],
                index=0 if default_type == "numeric" else 1,
                key=f"coltype_{col}"
            )
            column_types[col] = col_type
            if col_type == "category":
                categorical_columns.append(col)

        # ===== æ¨¡å‹èˆ‡è¶…åƒæ•¸ =====
        model_choice = st.selectbox(
            "ğŸ§  æ¨¡å‹é¸æ“‡ï¼ˆAuto = è‡ªå‹•æŒ‘æœ€ä½³æ¨¡å‹ï¼‰",
            ["auto", "lgbm", "xgboost", "catboost", "rf", "extra_tree", "lrl1", "sgd"]
        )

        custom_params = {}
        if model_choice != "auto":
            st.markdown("ğŸ›  è‡ªè¨‚æ¨¡å‹åƒæ•¸")
            learning_rate = st.number_input("learning_rate", value=0.1, step=0.01)
            n_estimators = st.number_input("n_estimators", value=100, step=10)
            max_depth = st.number_input("max_depth", value=6, step=1)
            custom_params = {
                "learning_rate": learning_rate,
                "n_estimators": int(n_estimators),
                "max_depth": int(max_depth)
            }

        # ===== å…¶ä»–è¨­å®š =====
        normalize = st.checkbox("âš™ï¸ å°æ•¸å€¼æ¬„ä½é€²è¡Œ MinMax æ­£è¦åŒ–", value=True)
        train_size = st.slider("ğŸ“ è¨“ç·´è³‡æ–™æ¯”ä¾‹", min_value=0.5, max_value=0.9, value=0.8, step=0.05)
        time_budget = st.number_input("â±ï¸ è¨“ç·´æ™‚é–“ä¸Šé™ï¼ˆç§’ï¼‰", value=60, min_value=10, step=10)

        if st.button("ğŸš€ é–‹å§‹è¨“ç·´ AutoML æ¨¡å‹"):
            try:
                # æº–å‚™è³‡æ–™
                X = df.drop(columns=[target]).copy()
                y = df[target]

                # å¥—ç”¨æ¬„ä½å‹åˆ¥è½‰æ›
                for col, col_type in column_types.items():
                    if col_type == "category":
                        X[col] = X[col].astype("category")
                    else:
                        X[col] = pd.to_numeric(X[col], errors="coerce")

                # One-hot encodingï¼ˆåªå°åˆ†é¡æ¬„ä½ï¼‰
                X_encoded = pd.get_dummies(X, columns=categorical_columns, prefix=categorical_columns)
                
                # å„²å­˜è™•ç†å¾Œçš„æ¬„ä½åç¨±
                final_columns = X_encoded.columns.tolist()

                # æ­£è¦åŒ–ï¼ˆåƒ…æ•¸å€¼æ¬„ä½ï¼‰
                scaler = None
                if normalize:
                    numeric_cols = X_encoded.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        scaler = MinMaxScaler()
                        X_encoded[numeric_cols] = scaler.fit_transform(X_encoded[numeric_cols])

                # train/test split
                X_train, X_test, y_train, y_test = train_test_split(
                    X_encoded, y, train_size=train_size, random_state=42, 
                    stratify=y if task == "classification" else None
                )

                # å»ºç«‹ AutoML
                automl = AutoML()
                log_file = st.session_state["log_file"]

                if model_choice == "auto":
                    automl.fit(
                        X_train=X_train, y_train=y_train,
                        task=task, time_budget=time_budget,
                        log_file_name=log_file
                    )
                else:
                    automl.fit(
                        X_train=X_train, y_train=y_train,
                        task=task, time_budget=time_budget,
                        estimator_list=[model_choice],
                        custom_hp=custom_params,
                        log_file_name=log_file
                    )

                # é æ¸¬
                y_pred = automl.predict(X_test)

                st.success("âœ… è¨“ç·´å®Œæˆï¼")
                if task == "classification":
                    acc = accuracy_score(y_test, y_pred)
                    st.metric("ğŸ¯ Accuracy", f"{acc:.4f}")
                else:
                    mse = mean_squared_error(y_test, y_pred)
                    st.metric("ğŸ“‰ MSE", f"{mse:.4f}")

                st.write("âœ… æœ€ä½³æ¨¡å‹ï¼š", automl.best_estimator)

                # === è¨“ç·´æ­·ç¨‹ ===
                st.subheader("ğŸ“‰ æ¨¡å‹è¨“ç·´éç¨‹ï¼ˆLearning Curveï¼‰")
                try:
                    time_hist, best_valid_loss_hist, valid_loss_hist, config_hist, metric_hist = \
                        get_output_from_log(filename=log_file, time_budget=time_budget)

                    score_hist = 1 - np.array(best_valid_loss_hist)

                    fig, ax = plt.subplots()
                    ax.step(time_hist, score_hist, where="post")
                    ax.set_xlabel("Wall Clock Time (s)")
                    ylabel = "Score (1 - best_valid_loss)"
                    if metric_hist and isinstance(metric_hist, dict) and "metric" in metric_hist:
                        metric_name = metric_hist["metric"]
                        if metric_name in {"accuracy", "macro_f1", "micro_f1", "f1", "roc_auc"}:
                            ylabel = f"Validation {metric_name}"
                    ax.set_ylabel(ylabel)
                    ax.set_title("Learning Curve")
                    st.pyplot(fig)
                except Exception as e:
                    st.info(f"å­¸ç¿’æ›²ç·šæš«ç„¡æ³•ç”¢ç”Ÿï¼ˆ{e}ï¼‰ã€‚")

                # === è¦–è¦ºåŒ–ï¼šåˆ†é¡ä»»å‹™ ===
                if task == "classification":
                    # æ··æ·†çŸ©é™£
                    st.subheader("ğŸ§© æ··æ·†çŸ©é™£")
                    fig, ax = plt.subplots()
                    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)
                    ax.set_title("Confusion Matrix")
                    st.pyplot(fig)

                    # ROCï¼ˆäºŒå…ƒï¼‰
                    classes_ = pd.Series(y_test).unique()
                    if len(classes_) == 2 and hasattr(automl, "predict_proba"):
                        try:
                            y_prob = automl.predict_proba(X_test)[:, 1]
                            fpr, tpr, _ = roc_curve(y_test, y_prob)
                            roc_auc = auc(fpr, tpr)

                            st.subheader("ğŸ“ˆ ROC æ›²ç·š")
                            fig, ax = plt.subplots()
                            ax.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
                            ax.plot([0, 1], [0, 1], linestyle="--")
                            ax.set_xlabel("False Positive Rate")
                            ax.set_ylabel("True Positive Rate")
                            ax.set_title("ROC Curve (Binary)")
                            ax.legend()
                            st.pyplot(fig)
                        except Exception as e:
                            st.info(f"ROC ç„¡æ³•ç¹ªè£½ï¼ˆ{e}ï¼‰ã€‚")

                # === ç‰¹å¾µé‡è¦æ€§ ===
                st.subheader("ğŸ” ç‰¹å¾µé‡è¦æ€§")
                feature_names = final_columns
                est = getattr(automl.model, "estimator", automl.model)
                importances = None
                try:
                    if hasattr(est, "feature_importances_"):
                        importances = np.array(est.feature_importances_).flatten()
                    elif hasattr(est, "coef_"):
                        importances = np.abs(np.array(est.coef_)).flatten()
                except Exception:
                    pass

                if importances is not None and len(importances) == len(feature_names):
                    sorted_idx = np.argsort(importances)
                    fig, ax = plt.subplots()
                    ax.barh(np.array(feature_names)[sorted_idx], importances[sorted_idx])
                    ax.set_xlabel("Importance")
                    ax.set_title("Feature Importance")
                    st.pyplot(fig)
                else:
                    st.info("æ­¤æœ€ä½³æ¨¡å‹ç„¡æ³•ç›´æ¥æä¾›ç‰¹å¾µé‡è¦æ€§ã€‚")

                # å„²å­˜è¨“ç·´çµæœåˆ° Sessionï¼ˆåŒ…å«å®Œæ•´çš„è™•ç†è³‡è¨Šï¼‰
                st.session_state["model"] = automl
                st.session_state["X_columns"] = final_columns
                st.session_state["scaler"] = scaler
                st.session_state["train_done"] = True
                st.session_state["column_types"] = column_types
                st.session_state["categorical_columns"] = categorical_columns

            except Exception as e:
                st.error(f"è¨“ç·´éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                st.exception(e)

# === ğŸ“¥ çµæœä¸‹è¼‰ ===
with tab2:
    st.header("ğŸ“¥ æ¨¡å‹èˆ‡ç‰¹å¾µä¸‹è¼‰")
    if st.session_state["train_done"]:
        # ä¸‹è¼‰ AutoMLï¼ˆåŒ…å«æœ€ä½³æ¨¡å‹ï¼‰
        model_bytes = io.BytesIO()
        joblib.dump(st.session_state["model"], model_bytes)
        model_bytes.seek(0)
        st.download_button("ğŸ’¾ ä¸‹è¼‰æ¨¡å‹ï¼ˆ.pklï¼‰", model_bytes, file_name="best_model.pkl")

        # ä¸‹è¼‰å®Œæ•´çš„è™•ç†è³‡è¨Šï¼ˆåŒ…å«æ¬„ä½åç¨±ã€å‹åˆ¥ã€åˆ†é¡æ¬„ä½ç­‰ï¼‰
        processing_info = {
            "X_columns": st.session_state["X_columns"],
            "column_types": st.session_state["column_types"],
            "categorical_columns": st.session_state["categorical_columns"]
        }
        
        info_bytes = io.BytesIO()
        json_str = json.dumps(processing_info, ensure_ascii=False, indent=2).encode("utf-8")
        info_bytes.write(json_str)
        info_bytes.seek(0)
        st.download_button("ğŸ“¥ ä¸‹è¼‰è™•ç†è³‡è¨Š JSON", info_bytes, file_name="processing_info.json")

        # ä¸‹è¼‰ scalerï¼ˆå¯é¸ï¼‰
        if st.session_state["scaler"] is not None:
            scaler_bytes = io.BytesIO()
            joblib.dump(st.session_state["scaler"], scaler_bytes)
            scaler_bytes.seek(0)
            st.download_button("ğŸ“¥ ä¸‹è¼‰æ•¸å€¼æ¨™æº–åŒ–å™¨ï¼ˆscaler.pklï¼‰", scaler_bytes, file_name="scaler.pkl")
        
        # é¡¯ç¤ºè³‡è¨Šé è¦½
        st.subheader("ğŸ“‹ è™•ç†è³‡è¨Šé è¦½")
        st.json(processing_info)
            
    else:
        st.info("âš ï¸ å°šæœªå®Œæˆè¨“ç·´")

# === ğŸ” æ¨¡å‹é æ¸¬ ===
with tab3:
    st.header("ğŸ” é æ¸¬å€")

    uploaded_model = st.file_uploader("ä¸Šå‚³è¨“ç·´å¥½æ¨¡å‹ï¼ˆ.pklï¼‰", type=["pkl"], key="upload_model")
    uploaded_info = st.file_uploader("ä¸Šå‚³è™•ç†è³‡è¨Š JSON", type=["json"], key="upload_info")
    uploaded_scaler = st.file_uploader("ï¼ˆå¯é¸ï¼‰ä¸Šå‚³ scaler.pkl", type=["pkl"], key="upload_scaler")

    if uploaded_model and uploaded_info:
        try:
            model = joblib.load(uploaded_model)
            processing_info = json.loads(uploaded_info.read().decode("utf-8"))
            
            # è¼‰å…¥è™•ç†è³‡è¨Š
            X_columns = processing_info["X_columns"]
            column_types = processing_info["column_types"] 
            categorical_columns = processing_info["categorical_columns"]
            
            st.session_state["model"] = model
            st.session_state["X_columns"] = X_columns
            st.session_state["column_types"] = column_types
            st.session_state["categorical_columns"] = categorical_columns

            if uploaded_scaler:
                st.session_state["scaler"] = joblib.load(uploaded_scaler)

            st.success("âœ… æ¨¡å‹èˆ‡è™•ç†è³‡è¨Šå·²è¼‰å…¥")
            st.json(processing_info)
            
        except Exception as e:
            st.error(f"è¼‰å…¥æ¨¡å‹æˆ–è™•ç†è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            st.exception(e)

    pred_file = st.file_uploader("ğŸ“¤ ä¸Šå‚³é æ¸¬ CSV", type=["csv"], key="predict_csv")

    if pred_file and st.session_state["model"] and st.session_state["X_columns"]:
        try:
            df_pred = pd.read_csv(pred_file)
            # æ¸…ç†æ¬„ä½åç¨±
            df_pred.columns = df_pred.columns.str.strip()
            
            st.subheader("ğŸ“Š åŸå§‹é æ¸¬è³‡æ–™é è¦½")
            st.dataframe(df_pred.head())

            # å–å¾—è™•ç†è³‡è¨Š
            column_types = st.session_state.get("column_types", {})
            categorical_columns = st.session_state.get("categorical_columns", [])
            
            # å¥—ç”¨æ¬„ä½å‹åˆ¥ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒçš„è™•ç†ï¼‰
            for col, typ in column_types.items():
                if col in df_pred.columns:
                    if typ == "category":
                        df_pred[col] = df_pred[col].astype("category")
                    else:
                        df_pred[col] = pd.to_numeric(df_pred[col], errors="coerce")

            # One-hot encodingï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒï¼‰
            df_pred_encoded = pd.get_dummies(df_pred, columns=[
                col for col in categorical_columns if col in df_pred.columns
            ], prefix=[col for col in categorical_columns if col in df_pred.columns])

            # ğŸ¯ é‡è¦ï¼šç¢ºä¿æ¬„ä½èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´
            X_columns = st.session_state["X_columns"]
            
            # æª¢æŸ¥ç¼ºå°‘çš„æ¬„ä½
            missing_cols = set(X_columns) - set(df_pred_encoded.columns)
            if missing_cols:
                st.warning(f"âš ï¸ é æ¸¬è³‡æ–™ç¼ºå°‘ä»¥ä¸‹æ¬„ä½ï¼Œå°‡ä»¥ 0 å¡«è£œï¼š{missing_cols}")
            
            # é‡æ–°ç´¢å¼•ä¸¦å¡«è£œç¼ºå°‘çš„æ¬„ä½
            df_pred_final = df_pred_encoded.reindex(columns=X_columns, fill_value=0)

            # å¼·åˆ¶è½‰ç‚º numeric
            df_pred_final = df_pred_final.apply(pd.to_numeric, errors="coerce").fillna(0)

            # å¥—ç”¨ scalerï¼ˆå¦‚æœ‰ï¼‰
            scaler = st.session_state.get("scaler", None)
            if scaler:
                numeric_cols = df_pred_final.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    df_pred_final[numeric_cols] = scaler.transform(df_pred_final[numeric_cols])

            st.subheader("ğŸ”§ è™•ç†å¾Œè³‡æ–™é è¦½")
            st.dataframe(df_pred_final.head())
            st.write(f"è™•ç†å¾Œå½¢ç‹€ï¼š{df_pred_final.shape}")
            st.write(f"é æœŸå½¢ç‹€ï¼š({len(df_pred)}, {len(X_columns)})")

            # é æ¸¬
            preds = st.session_state["model"].predict(df_pred_final)
            
            # å»ºç«‹çµæœ DataFrame
            result_df = df_pred.copy()  # ä½¿ç”¨åŸå§‹è³‡æ–™
            result_df["é æ¸¬å€¼"] = preds

            st.subheader("ğŸ“‹ é æ¸¬çµæœ")
            st.dataframe(result_df)

            # ä¸‹è¼‰çµæœ
            result_csv = result_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button("ğŸ“¥ ä¸‹è¼‰é æ¸¬çµæœ CSV", result_csv, file_name="predictions.csv")

            # Debug è³‡è¨Š
            with st.expander("ğŸ” Debug è³‡è¨Š"):
                st.write("**è¨“ç·´æ™‚æ¬„ä½ï¼š**")
                st.code(str(X_columns))
                st.write("**é æ¸¬è³‡æ–™æ¬„ä½ï¼š**") 
                st.code(str(df_pred_final.columns.tolist()))
                st.write("**é æ¸¬è³‡æ–™ dtypesï¼š**")
                st.code(str(df_pred_final.dtypes))

        except Exception as e:
            st.error(f"é æ¸¬éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            st.exception(e)